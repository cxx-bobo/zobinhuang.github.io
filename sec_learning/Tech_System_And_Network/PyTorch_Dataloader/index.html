<!DOCTYPE html>
<html lang="cn">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.zobinhuang.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":180,"display":"hide","padding":10,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#FF4136","save":"manual"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:type" content="website">
<meta property="og:title" content="PyTorch Dataset &amp; DataLoader 源码分析">
<meta property="og:url" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/index.html">
<meta property="og:site_name" content="Zobin">
<meta property="og:description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:locale">
<meta property="og:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/workflow.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/cooperate.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/map_style.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/iter_style.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/dataloader_wf.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/dataloader_wf_1.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/collate.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/single_process.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/pic/xxx.png">
<meta property="article:published_time" content="2022-09-23T17:20:55.657Z">
<meta property="article:modified_time" content="2022-09-23T17:20:55.657Z">
<meta property="article:author" content="Zhuobin Huang">
<meta property="article:tag" content="Zobin">
<meta property="article:tag" content="黄卓彬">
<meta property="article:tag" content="zobinHuang">
<meta property="article:tag" content="网络工程">
<meta property="article:tag" content="Networking Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">

<link rel="canonical" href="http://www.zobinhuang.com:10082/sec_learning/Tech_System_And_Network/PyTorch_Dataloader/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'cn'
  };
</script>

  <title>PyTorch Dataset & DataLoader 源码分析 | Zobin
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Zobin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="تشغيل شريط التصفح">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zobin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Loves Tech & Tea</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-关于我">

    <a href="/sec_about/" rel="section"><i class="fa fa-address-card fa-fw"></i>关于我</a>

  </li>
        <li class="menu-item menu-item-知识库">

    <a href="/sec_learning/" rel="section"><i class="fa fa-book-open fa-fw"></i>知识库</a>

  </li>
        <li class="menu-item menu-item-进度">

    <a href="/sec_schedule/" rel="section"><i class="fa fa-calendar-alt fa-fw"></i>进度</a>

  </li>
        <li class="menu-item menu-item-独立音乐人">

    <a href="/sec_music/" rel="section"><i class="fa fa-music fa-fw"></i>独立音乐人</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="cn">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">PyTorch Dataset & DataLoader 源码分析
</h1>

<div class="post-meta">
  
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_System_And_Network/">TECH_SYSTEM_AND_NETWORK</a></li>
          <li>PYTORCH_DATALOADER</li>
        
  </ul>

</div>

</header>

      
      
      
      <div class="post-body">
          <head>
<!--导入样式表-->
<link rel="stylesheet" type="text/css" href="style/index.css">

<!--导入网页脚本-->
<script src="script/index.js"></script>

<!--支持伪代码显示-->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-chtml.js"
        integrity="sha256-3Fdoa5wQb+JYfEmTpQHx9sc/GuwpfC/0R9EpBki+mf8=" crossorigin>
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>

<!--支持网页公式显示-->    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<!--支持矩阵显示-->
<script type="text/javascript">
  run_maths = function() {
    if (document.querySelector('[class*="cmath"]') !== null) {
      if (typeof (mjax_path)=='undefined') { mjax_path='https://cdn.jsdelivr.net/npm/mathjax@2'; }
      if (typeof (mjax_config)=='undefined') { mjax_config='AM_CHTML'; }
      smjax = document.createElement ('script');
      smjax.setAttribute('src',`${mjax_path}/MathJax.js?config=${mjax_config}`);
      smjax.setAttribute('async',true);
      document.getElementsByTagName('head')[0].appendChild(smjax);
    }
  };
  if (document.readyState === 'loading') {  
    window.addEventListener('DOMContentLoaded', run_maths); 
  } else { 
    run_maths(); 
  }
</script>
</head>

<body onload="load_page()">

<!-- 导入 mermaid -->
<script src="script/mermaid.min.js"></script>
<script>mermaid.initialize({startOnLoad:true});</script>

<div align="center" class="div_indicate_source">
  <h4>⚠ 转载请注明出处：<font color="red"><i>作者：ZobinHuang，更新日期：Sept.19 2022</i></font></h4>
</div>
<div class="div_licence">
  <br>
  <div align="center">
      <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="知识共享许可协议" style="border-width:0; margin-left: 20px; margin-right: 20px;" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a>
  </div>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;本<span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" rel="dct:type">作品</span>由 <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"><b>ZobinHuang</b></span> 采用 <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><font color="red">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</font></a> 进行许可，在进行使用或分享前请查看权限要求。若发现侵权行为，会采取法律手段维护作者正当合法权益，谢谢配合。
  </p>
</div>
<br>
<div class="div_catalogue">
  <div align="center">
    <h1> 目录 </h1>
    <p>
    <font size="3px">有特定需要的内容直接跳转到相关章节查看即可。</font>
  </div>
  <div class="div_load_catalogue_alert" id="load_catalogue_alert">正在加载目录...</div>
  <div class="div_catalogue_container" id="catalogue_container">
  </div>
</div><br>

<!-- Start your post here -->
<h2 class="title">基本使用与概念</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在本文中，我们将对 <code>torch.utils.data</code> 模块中的 $\text{Dataset}$，$\text{Sampler}$ 和 $\text{DataLoader}$ 三个用于 PyTorch 框架数据加载的关键类进行分析。


  <h3 class="title">数据加载</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先，模型的训练基于数据集，因此在训练过程中，有一部分的工作是要关心数据集如何从磁盘中被加载到 Host Memory / GPU Memory，数据集如何进行采样生成 Mini-batch，生成的 Mini-batch 如何被依次送进模型中进行训练，这也就是本文要关心的训练过程中被称为 <def>Dataloading</def> 的环节。

  <div class="img" title="训练流程中的 DataSet, DataLoader 和 Sampler" label="img_dataloading">
    <img src="./pic/workflow.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<imgref>img_dataloading</imgref> 展示了 Dataloading 在整个训练过程中所处的位置。首先，我们的数据集被存放在磁盘中，$\text{Dataset}$ 实体用于将 Dataset 从磁盘中读取到 Host Memory 中，并且提供了用于访问 Dataset 中各条 Sample 的 Feature 和 Label 的方法。在训练过程中，会有多轮 Epoches，每一轮 Epoch 会有多轮 Iterations，每一轮 Iteration 一个 Mini-batch 送入模型进行前向传播、损失值计算、参数梯度计算和参数更新，在每一轮 Epoch 中会完成一次对训练集 (Mini-batches) 的遍历。为了更好的实现对 Mini-batches 的遍历的编程抽象，$\text{DataLoader}$ 实体应该提供一种 <def>Iterable (可迭代)</def> 的方法，使得我们在每轮 Epoch 中可以基于 Python <code>for mini-batch in DataLoader</code> 的范式完成对 Mini-batches 的遍历。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;$\text{DataLoader}$ 在为某轮 Epoch 生成 Mini-batches 的过程中，是按照什么规则生成 Mini-batches 的呢？可以是按顺序在 $\text{Dataset}$ 中进行采样，也可以是按照随机的规则进行采样，生成 Mini-batches 的采样规则就是由 $\text{Sampler}$ 实体予以实现的。$\text{Sampler}$ 实体基于一定的采样规则，把采样生成的 Samples 的索引返回给 $\text{DataLoader}$，最后由 $\text{DataLoader}$ 完成从 $\text{Dataset}$ 中提取序号对应的 Samples 下的特征和标签数据，以最终完成 Mini-batches 的生成。

  <div class="theorm_prove">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;综上，$\text{Dataset}$，$\text{Sampler}$ 和 $\text{DataLoader}$ 可以总结为:

  <ul>
    <li><def>$\text{Dataset}$</def>: 将原始存储在磁盘中的数据集读取到内存中并封装为 Python 对应的对象，并且暴露提取接口;</li>
    <li><def>$\text{Sampler}$</def>: 决定采样方式，虽然 $\text{Dataloader}$ 是可以基于 $\text{Dataset}$ 提供的接口从数据集中提取 Samples 了，还是需要设置 $\text{Sampler}$ 告诉 $\text{Dataloader}$ 提取 Samples 的策略;</li>
    <li><def>$\text{Dataloader}$</def>: 基于设置好的 $\text{Dataset}$ 和 $\text{Sampler}$ 实体，再加上用户设置的 <code>shuffle</code>, <code>batch_size</code> 等参数，$\text{Dataloader}$ 将完成对应 Samples 的特征和标签数据的组装，以输出当前 Epoch 中使用的若干 Mini-batches。</li>
  </ul>

  <div class="img" title="DataLoader, Sampler 和 Dataset 三者关系图">
    <img src="./pic/cooperate.png" width="700px" />
  </div>

  </div>

  <h3 class="title">Python 中的迭代</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;理解 Python 中与迭代相关的概念，是理解 <code>torch.utils.data</code> 模块中的 $\text{Dataset}$，$\text{Sampler}$ 和 $\text{DataLoader}$ 三个实体的关键，因此如果读者朋友对 Python 中与迭代相关的概念尚不熟悉，建议先对我的另一篇文章 <a href="/sec_learning/Tech_Program/Python_Iteration/index.html">Python 中的迭代</a> 进行阅读。

  <h3 class="title">Python 中的多进程</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们在后面介绍 $\text{DataLoader}$ 实体的相关内容的时候将会涉及到如何基于 Python 的多进程机制加速数据加载的过程，因此需要读者对 Python 的多进程机制有一定的了解。如果您对此不是特别熟悉，建议先对我的另一篇文章 <a href="/sec_learning/Tech_Program/Python_Multiprocessing/index.html">Python 的多进程</a> 进行阅读。
</div> 

<h2 class="title">Dataset 功能性介绍</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;$\text{Dataset}$ 实体负责对来源于磁盘的 Raw Dataset 进行封装，将其封装成 Python 可识别的数据结构。<code>torch.utils.data</code> 模块提供了多种形式的 $\text{Dataset}$ 实体抽象，并且分别提供了对应的接口类完成对这些类型的数据集的抽象，用户需要实现接口类中的相应接口，以完成对自定义数据集的封装。我们下面对这些数据集抽象分别进行分析。

  <div class="multi_img">
    <div class="img" title="Map-style Dataset" label="map_style_ds">
      <img src="./pic/map_style.png" height="400px" />
    </div>
    <div class="img" title="Iterable-style Dataset" label="iter_style_ds">
      <img src="./pic/iter_style.png" height="400px" />
    </div>
  </div>

  <h3 class="paragraph">Map-style Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>torch.utils.data</code> 模块使用 <code>Dataset</code> 接口类抽象 Map-style 的数据集，其定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>) -&gt; T_co:</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__add__</span>(<span class="params">self, other: <span class="string">&#x27;Dataset[T_co]&#x27;</span></span>) -&gt; &#x27;ConcatDataset[T_co]&#x27;:</span></span><br><span class="line">      <span class="keyword">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Map-style 的数据集是一种通过实现 <code>__getitem__()</code> 和 <code>__len()__</code> 来获取数据的 Dataset，它实现了从索引/关键字到数据样本的映射。在访问这样的数据集时，使用 <code>dataset[idx]</code> 就可以访问 <code>idx</code> 对应的数据。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;细心的读者会发现在上面展示的 <code>Dataset</code> 接口类中并没有规定实现 <code>__len()__</code> 接口，原因是 <code>return NotImplemented</code> 或者 <code>raise NotImplementedError()</code> 之类的默认实现都会存在各自的问题，因此 <code>Dataset</code> 接口类把对 <code>__len()__</code> 接口的实现留给了子类。

  <h3 class="paragraph">Iterable-style Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Iterable-style 的数据集是一种通过实现 <code>__iter__()</code> 来获取数据的 Dataset，这种类型的数据集特别适用于以下情况: 随机读取代价很大甚至不大可能 (因此需要通过迭代器顺序获取 Samples)，且 batch size 取决于获取的数据。<code>torch.utils.data</code> 模块使用 <code>IterableDataset</code> 接口类抽象 Iterable-style 的数据集，其定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IterableDataset</span>(<span class="params">Dataset[T_co]</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[T_co]:</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__add__</span>(<span class="params">self, other: Dataset[T_co]</span>):</span></span><br><span class="line">      <span class="keyword">return</span> ChainDataset([self, other])</span><br></pre></td></tr></table></figure>
  <h3 class="paragraph">Concat Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>ConcatDataset</code> 被用于级联多个数据集类，使得级联出来的数据集就像是一个统一大的数据集一样，可以基于索引/关键字对 Samples 进行访问，具体定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConcatDataset</span>(<span class="params">Dataset[T_co]</span>):</span></span><br><span class="line">  datasets: List[Dataset[T_co]]</span><br><span class="line">  cumulative_sizes: List[<span class="built_in">int</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">  @staticmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cumsum</span>(<span class="params">sequence</span>):</span></span><br><span class="line">      r, s = [], <span class="number">0</span></span><br><span class="line">      <span class="keyword">for</span> e <span class="keyword">in</span> sequence:</span><br><span class="line">          l = <span class="built_in">len</span>(e)</span><br><span class="line">          r.append(l + s)</span><br><span class="line">          s += l</span><br><span class="line">      <span class="keyword">return</span> r</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, datasets: Iterable[Dataset]</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      <span class="built_in">super</span>(ConcatDataset, self).__init__()</span><br><span class="line">      self.datasets = <span class="built_in">list</span>(datasets)</span><br><span class="line">      <span class="keyword">assert</span> <span class="built_in">len</span>(self.datasets) &gt; <span class="number">0</span>, <span class="string">&#x27;datasets should not be an empty iterable&#x27;</span>  <span class="comment"># type: ignore[arg-type]</span></span><br><span class="line">      <span class="keyword">for</span> d <span class="keyword">in</span> self.datasets:</span><br><span class="line">          <span class="keyword">assert</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(d, IterableDataset), <span class="string">&quot;ConcatDataset does not support IterableDataset&quot;</span></span><br><span class="line">      self.cumulative_sizes = self.cumsum(self.datasets)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> self.cumulative_sizes[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">      <span class="keyword">if</span> idx &lt; <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">if</span> -idx &gt; <span class="built_in">len</span>(self):</span><br><span class="line">              <span class="keyword">raise</span> ValueError(<span class="string">&quot;absolute value of index should not exceed dataset length&quot;</span>)</span><br><span class="line">          idx = <span class="built_in">len</span>(self) + idx</span><br><span class="line">      dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)</span><br><span class="line">      <span class="keyword">if</span> dataset_idx == <span class="number">0</span>:</span><br><span class="line">          sample_idx = idx</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          sample_idx = idx - self.cumulative_sizes[dataset_idx - <span class="number">1</span>]</span><br><span class="line">      <span class="keyword">return</span> self.datasets[dataset_idx][sample_idx]</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cummulative_sizes</span>(<span class="params">self</span>):</span></span><br><span class="line">      warnings.warn(<span class="string">&quot;cummulative_sizes attribute is renamed to &quot;</span></span><br><span class="line">                    <span class="string">&quot;cumulative_sizes&quot;</span>, DeprecationWarning, stacklevel=<span class="number">2</span>)</span><br><span class="line">      <span class="keyword">return</span> self.cumulative_sizes</span><br></pre></td></tr></table></figure>
  <h3 class="paragraph">Chain Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>ChainDataset</code> 被用于包含多个 <code>IterableDataset</code> 数据集，在 <code>IterableDataset</code> 的 <code>__add__()</code> 方法中被调用，具体定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChainDataset</span>(<span class="params">IterableDataset</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, datasets: Iterable[Dataset]</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      <span class="built_in">super</span>(ChainDataset, self).__init__()</span><br><span class="line">      self.datasets = datasets</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">for</span> d <span class="keyword">in</span> self.datasets:</span><br><span class="line">          <span class="keyword">assert</span> <span class="built_in">isinstance</span>(d, IterableDataset), <span class="string">&quot;ChainDataset only supports IterableDataset&quot;</span></span><br><span class="line">          <span class="keyword">for</span> x <span class="keyword">in</span> d:</span><br><span class="line">              <span class="keyword">yield</span> x</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      total = <span class="number">0</span></span><br><span class="line">      <span class="keyword">for</span> d <span class="keyword">in</span> self.datasets:</span><br><span class="line">          <span class="keyword">assert</span> <span class="built_in">isinstance</span>(d, IterableDataset), <span class="string">&quot;ChainDataset only supports IterableDataset&quot;</span></span><br><span class="line">          total += <span class="built_in">len</span>(d)  <span class="comment"># type: ignore[arg-type]</span></span><br><span class="line">      <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure>
  <h3 class="paragraph">Subset Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>Subset</code> 被用于将原有数据集指定下标的 Samples 封装为一个新的数据集，具体定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Subset</span>(<span class="params">Dataset[T_co]</span>):</span></span><br><span class="line">  dataset: Dataset[T_co]</span><br><span class="line">  indices: Sequence[<span class="built_in">int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset: Dataset[T_co], indices: Sequence[<span class="built_in">int</span>]</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      self.dataset = dataset</span><br><span class="line">      self.indices = indices</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">isinstance</span>(idx, <span class="built_in">list</span>):</span><br><span class="line">          <span class="keyword">return</span> self.dataset[[self.indices[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx]]</span><br><span class="line">      <span class="keyword">return</span> self.dataset[self.indices[idx]]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">len</span>(self.indices)</span><br></pre></td></tr></table></figure>
  <h3 class="paragraph">Tensor Dataset</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>TensorDataset</code> 用于获取封装成 Tensor 的数据集，每一个样本都通过索引张量来获得，具体代码如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TensorDataset</span>(<span class="params">Dataset[Tuple[Tensor, ...]]</span>):</span></span><br><span class="line">  tensors: Tuple[Tensor, ...]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *tensors: Tensor</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      <span class="keyword">assert</span> <span class="built_in">all</span>(tensors[<span class="number">0</span>].size(<span class="number">0</span>) == tensor.size(<span class="number">0</span>) <span class="keyword">for</span> tensor <span class="keyword">in</span> tensors), <span class="string">&quot;Size mismatch between tensors&quot;</span></span><br><span class="line">      self.tensors = tensors</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">tuple</span>(tensor[index] <span class="keyword">for</span> tensor <span class="keyword">in</span> self.tensors)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> self.tensors[<span class="number">0</span>].size(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</div>

<h2 class="title">Sampler 功能性介绍</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在我们定义好了 $\text{Dataset}$ 实体抽象，已经拥有了一些接口可以获取 $\text{Dataset}$ 中各条 Samples。在训练的各轮 Epoches 中，我们需要在每轮 Iteration 中从 $\text{Dataset}$ 中读取单条 Sample (或多条 Samples 以形成 Mini-batch) 对模型进行训练，那么应该按照什么顺序来读取 $\text{Dataset}$ 中的内容以形成这些 Mini-batches 呢？这也就是我们本节要讨论的 $\text{Sampler}$ 实体抽象的工作。

  <noteblock>
  在每轮 Iteration 中，我们把每次输出一条 Sample 的 Index 的实体称为 $\text{Sampler}$；把每次输出多条 Samples 的 Indices 的实体称为 $\text{Batch Sampler}$。
  </noteblock>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style Dataset 来说，如 <imgref>map_style_ds</imgref> 所示，每一条 Sample 都会有一个索引/键值，$\text{Sampler}$ ($\text{Batch Sampler}$) 的功能就是在每轮 Epoch 的每轮 Iteration 中输出参与训练的单条 Sample (多条 Samples) 的 索引/键值，具体的做法是在每轮 Iteration 的一开始 <code>yield</code> 出一个 (多个) 索引/键值。值得注意的是 $\text{Sampler}$ 只负责输出参与训练的单条 Sample (多条 Samples) 的索引/键值，而真正加载这些 Samples 的特征和标签数据的工作则是由我们后面要介绍的 $\text{DataLoader}$ 完成的。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;而对于 Iteration-style Dataset 来说，如 <imgref>iter_style_ds</imgref> 所示，每一条 Sample 并不会有索引/键值。正如我们上面所述，Iteration-style Dataset 需要基于 Iterator 的方法来完成对数据集中的 Samples 的访问，因此对 Iteration-style Dataset 中 Samples 的遍历顺序完全由其 <code>__iter__(self)</code> 魔法方法定义的访问顺序决定，因此本节讨论的 $\text{Sampler}$ 实体并不对 Iteration-style Dataset 有效，因此我们默认下文讨论的都是 Map-style Dataset。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 <code>torch.utils.data</code> 模块中，<code>Sampler</code> 类定义了 $\text{Sampler}$ 实体所应该拥有的接口，具体定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sampler</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line">  <span class="string">r&quot;&quot;&quot;Base class for all Samplers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Every Sampler subclass has to provide an :meth:`__iter__` method, providing a</span></span><br><span class="line"><span class="string">  way to iterate over indices of dataset elements, and a :meth:`__len__` method</span></span><br><span class="line"><span class="string">  that returns the length of the returned iterators.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  .. note:: The :meth:`__len__` method isn&#x27;t strictly required by</span></span><br><span class="line"><span class="string">            :class:`~torch.utils.data.DataLoader`, but is expected in any</span></span><br><span class="line"><span class="string">            calculation involving the length of a :class:`~torch.utils.data.DataLoader`.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_source: Optional[Sized]</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[T_co]:</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;从上面的代码和注释中可以看到，$\text{Sampler}$ 实体需要定义 <code>__iter__(self)</code> 魔法方法，以可以通过迭代的方法来访问 $\text{Sampler}$ 采样得到的单条 Sample 的序号，以供后续 $\text{DataLoader}$ 从 $\text{Dataset}$ 中完成对应序号的特征和标签数据的读取和组装。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;$\text{Sampler}$ 实体将会作为一个初始化参数传给我们后面要介绍的 $\text{DataLoader}$ 实体，后者在读取数据的时候将会调用 $\text{Sampler}$ 的相关接口以获得采样得到的 Sample。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外根据注释，$\text{Sampler}$ 实体的 <code>__len__(self)</code> 魔法方法不是必要的，但是当 $\text{DataLoader}$ 实体在其具体实现中需要计算 $\text{DataLoader}$ 实体的 <code>len()</code> 的时候必须定义，这点在其源码中也有注释加以体现。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 <code>Sampler</code> 接口类，<code>torch.utils.data</code> 模块中提供了多种内置的 $\text{Sampler}$，下面展示了其中的 <code>RandomSampler</code> 的相关定义。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomSampler</span>(<span class="params">Sampler[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">  <span class="string">r&quot;&quot;&quot;Samples elements randomly. If without replacement, then sample from a shuffled dataset.</span></span><br><span class="line"><span class="string">  If with replacement, then user can specify :attr:`num_samples` to draw.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">      data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">      replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``</span></span><br><span class="line"><span class="string">      num_samples (int): number of samples to draw, default=`len(dataset)`.</span></span><br><span class="line"><span class="string">      generator (Generator): Generator used in sampling.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  data_source: Sized</span><br><span class="line">  replacement: <span class="built_in">bool</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_source: Sized, replacement: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_samples: Optional[<span class="built_in">int</span>] = <span class="literal">None</span>, generator=<span class="literal">None</span></span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      self.data_source = data_source</span><br><span class="line">      self.replacement = replacement</span><br><span class="line">      self._num_samples = num_samples</span><br><span class="line">      self.generator = generator</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.replacement, <span class="built_in">bool</span>):</span><br><span class="line">          <span class="keyword">raise</span> TypeError(<span class="string">&quot;replacement should be a boolean value, but got &quot;</span></span><br><span class="line">                          <span class="string">&quot;replacement=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.replacement))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.num_samples, <span class="built_in">int</span>) <span class="keyword">or</span> self.num_samples &lt;= <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">raise</span> ValueError(<span class="string">&quot;num_samples should be a positive integer &quot;</span></span><br><span class="line">                           <span class="string">&quot;value, but got num_samples=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.num_samples))</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">num_samples</span>(<span class="params">self</span>) -&gt; int:</span></span><br><span class="line">      <span class="comment"># dataset size might change at runtime</span></span><br><span class="line">      <span class="keyword">if</span> self._num_samples <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="built_in">len</span>(self.data_source)</span><br><span class="line">      <span class="keyword">return</span> self._num_samples</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[int]:</span></span><br><span class="line">      n = <span class="built_in">len</span>(self.data_source)</span><br><span class="line">      <span class="keyword">if</span> self.generator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          seed = <span class="built_in">int</span>(torch.empty((), dtype=torch.int64).random_().item())</span><br><span class="line">          generator = torch.Generator()</span><br><span class="line">          generator.manual_seed(seed)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          generator = self.generator</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self.replacement:</span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_samples // <span class="number">32</span>):</span><br><span class="line">              <span class="keyword">yield</span> <span class="keyword">from</span> torch.randint(high=n, size=(<span class="number">32</span>,), dtype=torch.int64, generator=generator).tolist()</span><br><span class="line">          <span class="keyword">yield</span> <span class="keyword">from</span> torch.randint(high=n, size=(self.num_samples % <span class="number">32</span>,), dtype=torch.int64, generator=generator).tolist()</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_samples // n):</span><br><span class="line">              <span class="keyword">yield</span> <span class="keyword">from</span> torch.randperm(n, generator=generator).tolist()</span><br><span class="line">          <span class="keyword">yield</span> <span class="keyword">from</span> torch.randperm(n, generator=generator).tolist()[:self.num_samples % n]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>) -&gt; int:</span></span><br><span class="line">      <span class="keyword">return</span> self.num_samples</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 <code>RandomSampler</code> 的 <code>__iter__(self)</code> 魔法方法中我们可以看到，其返回的实际上是一个 Generator，采用 on-the-fly 的方式，随机地，可支持有放回地生成 Sample 的采样序号。
</div>

<h2 class="title">DataLoader 功能性介绍</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;铺垫完了 $\text{Dataset}$ 和 $\text{Sampler}$ 两个实体，现在我们来到了数据加载流程的核心 —— $\text{DataLoader}$。<code>torch.utils.data</code> 提供的 <code>DataLoader</code> 是 PyTorch 加载数据的核心，其同时支持 Map-style 和 Iterable-style 的 $\text{Dataset}$，支持单进程/多进程数据加载，还可以设置 loading order, batch size, pin memory 等加载参数的设置。

  <h3 class="title">DataLoader 接口</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>DataLoader</code> 的接口定义如下：

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(</span><br><span class="line">  dataset: Dataset[T_co],</span><br><span class="line">  batch_size: Optional[<span class="built_in">int</span>] = <span class="number">1</span>,</span><br><span class="line">  shuffle: Optional[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span><br><span class="line">  sampler: Union[Sampler, Iterable, <span class="literal">None</span>] = <span class="literal">None</span>,</span><br><span class="line">  batch_sampler: Union[Sampler[Sequence], Iterable[Sequence], <span class="literal">None</span>] = <span class="literal">None</span>,</span><br><span class="line">  num_workers: <span class="built_in">int</span> = <span class="number">0</span>,</span><br><span class="line">  collate_fn: Optional[_collate_fn_t] = <span class="literal">None</span>,</span><br><span class="line">  pin_memory: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">  drop_last: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">  timeout: <span class="built_in">float</span> = <span class="number">0</span>,</span><br><span class="line">  worker_init_fn: Optional[_worker_init_fn_t] = <span class="literal">None</span>,</span><br><span class="line">  multiprocessing_context=<span class="literal">None</span>,</span><br><span class="line">  generator=<span class="literal">None</span>,</span><br><span class="line">  *,</span><br><span class="line">  prefetch_factor: <span class="built_in">int</span> = <span class="number">2</span>,</span><br><span class="line">  persistent_workers: <span class="built_in">bool</span> = <span class="literal">False</span>,</span><br><span class="line">  pin_memory_device: <span class="built_in">str</span> = <span class="string">&quot;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;接口参数整理如下所示:

  <div class="table" title="DataLoader 接口参数说明">
  <table>
    <tr>
      <th align="center">Attribute</th>
      <th align="center">Description</th>
      <th align="center">Default Value</th>
      <th align="center">Type</th>
    </tr>
    <tr>
      <td><code>dataset</code></td>
      <td>要加载的 $\text{Dataset}$ 实体</td>
      <td></td>
      <td><code>Dataset</code></td>
    </tr>
    <tr>
      <td><code>batch_size</code></td>
      <td>每个 Mini-batch 要加载的 Samples 的数目</td>
      <td>$1$</td>
      <td><code>int</code></td>
    </tr>
    <tr>
      <td><code>shuffle</code></td>
      <td>设置为 <code>True</code> 时，将调用 <code>RandomSampler</code> 进行随机索引</td>
      <td><code>False</code></td>
      <td><code>bool</code></td>
    </tr>
    <tr>
      <td><code>sampler</code></td>
      <td>用户指定的 $\text{Sampler}$ 实体，定义从 <note>Map-style</note> Dataset 中提取样本的策略，每一次 <code>yield</code> <note>一条</note> Sample 的索引/键值。如果指定了, <code>shuffle</code> 参数必须为 <code>False</code>，否则会和 <code>RandomSampler</code> 互斥</td>
      <td><code>None</code></td>
      <td><code>Sampler</code>, <code>Iterable</code></td>
    </tr>
    <tr>
      <td><code>batch_sampler</code></td>
      <td>用户指定的 $\text{Batch Sampler}$ 实体，定义从 <note>Map-style</note> Dataset 中提取样本的策略，每一次 <code>yield</code> <note>多条</note> Samples 的索引/键值</td>
      <td><code>None</code></td>
      <td><code>Sampler</code>, <code>Iterable</code></td>
    </tr>
    <tr>
      <td><code>num_workers</code></td>
      <td>要用于数据加载的子进程数，$0$ 表示将在主进程中加载数据</td>
      <td>$0$</td>
      <td><code>int</code></td>
    </tr>
    <tr>
      <td><code>collate_fn</code></td>
      <td>在将 Map-style Dataset 取出的数据整合成 Mini-batch 时使用，合并样本列表以形成一个 Mini-batch</td>
      <td><code>None</code></td>
      <td><code>callable</code></td>
    </tr>
    <tr>
      <td><code>pin_memory</code></td>
      <td>如果为 <code>True</code>，则 <code>DataLoader</code> 在将张量返回之前将其复制到 CUDA 固定的内存中</td>
      <td><code>False</code></td>
      <td><code>bool</code></td>
    </tr>
    <tr>
      <td><code>drop_last</code></td>
      <td>若设置为 <code>True</code>，则当该数据集大小 (i.e. <code>len(dataset)</code>) 不能被该批次大小 (i.e. <code>batch_size</code>) 整除时，删除最后一个不完整的批次；如果为 <code>False</code> 并且数据集的大小不能被批次大小整除，那么最后一批将较小</td>
      <td><code>False</code></td>
      <td><code>bool</code></td>
    </tr>
    <tr>
      <td><code>timeout</code></td>
      <td>如果为正数，则为从 Worker 收集 Mini-batch 的超时值，应始终为非负数。超过这个时间还没从 Worker 读取到数据的话就会报错</td>
      <td>$0$</td>
      <td><code>numeric</code></td>
    </tr>
    <tr>
      <td><code>worker_init_fn</code></td>
      <td>如果不为 <code>None</code>，它将会被每个 Worker 子进程调用。该函数将以 Worker Index (i.e. [0, <code>num_workers</code> - 1] 内的整形) 作为输入</td>
      <td><code>None</code></td>
      <td><code>callable</code></td>
    </tr>
    <tr>
      <td><code>prefetch_factor</code></td>
      <td>每个 Worker 提前加载的 Sample 数量</td>
      <td>$2$</td>
      <td><code>int</code></td>
    </tr>
    <tr>
      <td><code>persistent_workers</code></td>
      <td>如果为 <code>True</code>，则 <code>Dataloader</code> 将不会终止 Worker 进程，直到对 $\text{Dataset}$ 的迭代完成</td>
      <td><code>False</code></td>
      <td><code>bool</code></td>
    </tr>
  </table>
  </div>

  <h3 class="title">自动化批处理 (Automatic Batching)</h3>
  <div class="img" title="自动批处理流程">
    <img src="./pic/dataloader_wf.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>DataLoader</code> 非常方便地为用户提供了 <def>自动化批处理 (Automatic Batching)</def> 的功能，通过指定我们上面看到的相关接口中的参数，用户可以自定义出他们想要的 $\text{DataLoader}$ 实体，然后在训练主进程中通过迭代的方式获取每轮 Iteration 指定规模和指定顺序的 Samples。

  <h4 class="paragraph">Batch Sampler 输出 Samples Indices</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 SGD 的训练思路，通常在一轮 Iteration 中会使用多个 Samples 组成的 Mini-batch 进行训练，而通常不会只有一条 Sample 参与训练，<code>DataLoader</code> 提供了相关的参数进行 $\text{Batch Sampler}$ 相关的设置:

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style Dataset 和 Iterable-style Dataset 来说，当 <code>batch_size</code> (默认为 $1$) 不为 <code>None</code> 时，生成的 $\text{DataLoader}$ 在每一次被迭代时将 <code>yield</code> 出一批 Samples，而不只是一个单独的 Sample，该参数与 <code>drop_last</code> 和 <code>shuffle</code> 参数配合，将决定每一轮 Iteration 所使用的 Samples 的顺序和数目。

  <div class="border">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;实际上我们后面会看到，在 <code>DataLoader</code> 的构造函数中，相关代码会基于 <code>batch_size</code> 和 <code>drop_last</code> 参数，结合用户指定的 $\text{Sampler}$ 实体构造出对应的 $\text{Batch Sampler}$ 实体。那么 $\text{Sampler}$ 实体是如何被指定的呢？对于 Map-style Dataset 来说，可以通过两种方式被指定: 一种是通过 <code>sampler</code> 参数显式指定 $\text{Sampler}$，一种是通过 <code>shuffle</code> 参数，当它为 <code>True</code> 时使用 <code>RandomSampler</code>; 而对于 Iterable-style Dataset 来说，会使用专门针对于 <code>Iterable-style Dataset</code> 提供的 Dummy Infinite Sampler <code>_InfiniteConstantSampler</code> 以充当 $\text{Sampler}$ 实体，该 $\text{Sampler}$ 可以无限地被迭代，其实质上是调用了 Iterable-style Dataset 的 <code>__iter__(self)</code> 魔法方法。
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;单独对于 Map-style Dataset 来说，用户还可以使用 <code>batch_sampler</code> 参数来直接设置 Mini-batch 的 $\text{Batch Sampler}$，该参数指定的 $\text{Batch Sampler}$ 实体在每次被迭代时将会 <code>yield</code> 出指定规模的 Indices。

  <h4 class="paragraph">基于 Samples Indices 合成 Mini-batch</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在基于 $\text{Batch Sampler}$ <code>yield</code> 出一批某轮 Iteration 使用的 Samples 的 Indices 后，接下来就需要由 <code>collate_fn</code> 参数指定的 <def>Collate Function (整理函数)</def> 基于指定的 Indices 合成由对应 Samples 形成的 Mini-batch。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style Dataset 来说，Collate Function 流程可以抽象如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> indices <span class="keyword">in</span> batch_sampler:</span><br><span class="line">    <span class="keyword">yield</span> collate_fn([dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices])</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Iteration-style Dataset 来说，Collate Function 流程可以抽象如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset_iter = <span class="built_in">iter</span>(dataset)</span><br><span class="line"><span class="keyword">for</span> indices <span class="keyword">in</span> batch_sampler:</span><br><span class="line">    <span class="keyword">yield</span> collate_fn([<span class="built_in">next</span>(dataset_iter) <span class="keyword">for</span> _ <span class="keyword">in</span> indices])</span><br></pre></td></tr></table></figure>
  <h3 class="title">关闭自动批处理</h3>
  <div class="img" title="关闭自动批处理流程">
    <img src="./pic/dataloader_wf_1.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当用户想用 $\text{Dataset}$ 的代码手动处理 Batch，或每轮 Iteration 仅基于单条 Sample 进行训练时，可将 <code>batch_size</code> 和 <code>batch_sampler</code> 两个参数同时设为 <code>None</code>, 此时 <code>DataLoader</code> 将关闭自动批处理，<code>DataLoader</code> 将通过迭代 $\text{Dataset}$ 获得 Sample(s)，然后将 Sample 交给 <code>collate_fn</code> 处理，以获得最终的 <code>DataLoader</code> 输出。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style Dataset 来说，Collate Function 流程可以抽象如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> sampler:</span><br><span class="line">    <span class="keyword">yield</span> collate_fn(dataset[index])</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Iteration-style Dataset 来说，Collate Function 流程可以抽象如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> <span class="built_in">iter</span>(dataset):</span><br><span class="line">    <span class="keyword">yield</span> collate_fn(data)</span><br></pre></td></tr></table></figure>
  <h3 class="title">Collate 函数</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当关闭自动批处理时，<code>collate_fn</code> 仅作用于单个 Sample，其工作就是简单地将 NumPy <code>arrays</code> 转化为 PyTorch 的 <code>Tensor</code>。

  <div class="img" title="开启自动批处理的 Collate 函数 (以各条 Sample 的形式是 dict 为例)" label="img_batch_collate" >
    <img src="./pic/collate.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;而当开启自动批处理时，<code>collate_fn</code> 作用于多个 Samples，其将输入样本整理为一个 Batch，并将其 <code>yield</code> 回当前轮次的 Itertaion 以供训练。为了将输入样本整理为一个 Batch，如 <imgref>img_batch_collate</imgref> 所示，<code>collate_fn</code> 的默认值 <code>default_collate()</code> 做了下面 $3$ 件事情:

  <ul>
    <li>追加 (Prepend) 一个新的维度作为 Batch Dimension (长度即为 Batch 的大小);</li>
    <li>将 NumPy <code>arrays</code> 和 Python Numberical Values 转化为 PyTorch 的 <code>Tensor</code>;</li>
    <li>保留输入的 Samples 中各条 Sample 的数据结构，如 <imgref>img_batch_collate</imgref> 所示，比如各条 Sample 是 <code>dict</code> 时，<code>default_collate()</code> 将输出具有相同 Keys，且处理过的 Batched Tensor (或 Batched List，当无法转化为 Tensor 的时候) 作为值的 <code>dict</code>。当各条 Sample 是 <code>list</code>、<code>tuple</code> 和 <code>namedtuple</code> 等时同理;</li>
  </ul>

  <h3 class="title">单进程数据加载</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当设置 <code>DataLoader</code> 的 <code>num_workers</code> 为 $0$ (默认值) 时，则 <code>DataLoader</code> 的初始化进程和读取数据的进程是一样的，此时数据加载可能会导致主进程阻塞。当用于在进程之间共享数据的资源 (例如共享内存，文件描述符) 有限时，或者当整个数据集很小并且可以完全加载到内存中时，此模式可能是首选。此外，单进程加载通常显示更多可读的错误跟踪，因此对于调试很有用。

  <h3 class="title">多进程数据加载</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在多进程模式下，每次对 <code>DataLoader</code> 进行迭代时 (e.g. 当调用 <code>enumerate(dataloader)</code> 时)，都会创建 <code>num_workers</code> 个 <def>工作进程 (Worker)</def>，<code>dataset</code>, <code>collate_fn</code>, <code>worker_init_fn</code> 等参数都会被传到各个 Worker 中，各个 Worker 使用这些参数进行初始化和数据的读取和整理。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<code>torch.utils.data</code> 模块提供了 <code>get_work_info</code> 函数用于在各个 Worker 进程中获取各个进程相关的信息，包括 Worker 的 ID，Dataset 的 Replica，以及 Initial Seed 等，在主进程而不是 Worker 进程中调用 <code>get_work_info</code> 函数将返回 <code>None</code>。用户可以在 $\text{Dataset}$ 的迭代代码以及 <code>worker_init_fn</code> 指明的 Worker 进程初始化函数中利用 <code>get_work_info</code> 函数返回的内容对 Dataset Replica 进行不同的处理。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Map-style 的 Dataset 来说，主进程将会利用 $\text{Sampler}$ ($\text{Batch Sampler}$) 实体生成 Indices，然后将生成的 Indices 发送给各个 Worker 进程，也即「选择 Sample」这件事情是在主进程完成的，而「根据选择结果加载数据」这件事情是在各个 Worker 进程中完成的。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 Iterable-style 的 Dataset 来说，主进程中将不会进行生成 Indices 的操作，在各个 Worker 进程中会有一份 Dataset 的 Replica，各个 Worker 进程可以基于 <code>get_work_info</code> 函数获得的信息，对各份 Replica 进行不同的操作。

  <h3 class="title">Pinned Memory (锁页内存)</h3>
  <noteblock>
  我的另一篇文章 <a href="/sec_learning/Tech_OS_And_Linux_Kernel/CUDA_Memory_Management/index.html">CUDA 内存管理</a> 对 CUDA Pinned Memory 相关内容进行了介绍
  </noteblock>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Host Memory 中的 Memory Page 有两种存在方式，一是 <def>锁页 (Pinned)</def>，二是 <def>不锁页 (Unpinned)</def>，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换，而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。主机在向 GPU 拷贝数据时，CUDA 要求数据必须放在 Pinned Host Memory 中，因此如果数据不在 Pinned Memory 中，会在 Host Memory 中经历从 Unpinned Memory 到 Pinned Memory 的拷贝过程。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pin_memory</span>(<span class="params">data, device=<span class="literal">None</span></span>):</span></span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, torch.Tensor):</span><br><span class="line">      <span class="keyword">return</span> data.pin_memory(device)</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, string_classes):</span><br><span class="line">      <span class="keyword">return</span> data</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, collections.abc.Mapping):</span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="built_in">type</span>(data)(&#123;k: pin_memory(sample, device) <span class="keyword">for</span> k, sample <span class="keyword">in</span> data.items()&#125;)  <span class="comment"># type: ignore[call-arg]</span></span><br><span class="line">      <span class="keyword">except</span> TypeError:</span><br><span class="line">          <span class="comment"># The mapping type may not support `__init__(iterable)`.</span></span><br><span class="line">          <span class="keyword">return</span> &#123;k: pin_memory(sample, device) <span class="keyword">for</span> k, sample <span class="keyword">in</span> data.items()&#125;</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="built_in">hasattr</span>(data, <span class="string">&#x27;_fields&#x27;</span>):  <span class="comment"># namedtuple</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">type</span>(data)(*(pin_memory(sample, device) <span class="keyword">for</span> sample <span class="keyword">in</span> data))</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">tuple</span>):</span><br><span class="line">      <span class="keyword">return</span> [pin_memory(sample, device) <span class="keyword">for</span> sample <span class="keyword">in</span> data]  <span class="comment"># Backwards compatibility.</span></span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, collections.abc.Sequence):</span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="built_in">type</span>(data)([pin_memory(sample, device) <span class="keyword">for</span> sample <span class="keyword">in</span> data])  <span class="comment"># type: ignore[call-arg]</span></span><br><span class="line">      <span class="keyword">except</span> TypeError:</span><br><span class="line">          <span class="comment"># The sequence type may not support `__init__(iterable)` (e.g., `range`).</span></span><br><span class="line">          <span class="keyword">return</span> [pin_memory(sample, device) <span class="keyword">for</span> sample <span class="keyword">in</span> data]</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">hasattr</span>(data, <span class="string">&quot;pin_memory&quot;</span>):</span><br><span class="line">      <span class="keyword">return</span> data.pin_memory()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;PyTorch 为存储在 Host Memory 中的 Tensor 提供了 <code>pin_memory()</code> 方法，其定义如上所示，该方法返回操作的 Tensor 的副本，并将数据放在 Pinned Memory 中。对于放在 Pinned Memory 中的 Tensor，用户可以使用 <def>异步 GPU 拷贝 (Asynchronous GPU copies</def> 以将 Tensor 拷贝至 GPU 内存中，做法是在 Tensor 的 <code>to()</code> 方法中加上参数 <code>non_blocking=True</code>，以实现数据传输和 Host 计算两者的 Overlapping <cite>torch_pinned_memory</cite>。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 <code>DataLoader</code> 来说，我们可以设置传入参数 <code>pin_memory=True</code>，以设置 <code>DataLoader</code> 将每次迭代返回的 Tensor 都放置到 Pinned Memory 中，以缩减数据在 Host Memory 和 GPU Memory 之间的拷贝时间。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外，从上面关于 <code>pin_memory()</code> 方法的代码定义中可以看到，如果传入该函数的是一个自定义的数据类型，则该函数会直接返回该数据，而不做任何 Pinning 相关的处理。对于 <code>DataLoader</code> 来说，当我们使用 <code>collate_fn</code> 指定了自定义的整理函数并且该整理函数返回了自定义类型的 Batch 数据，则当我们指定 <code>DataLoader</code> 的 <code>pin_memory=True</code> 时，则会导致 Memory Pinning 的操作并不会生效的情况。为了解决这种情况，我们需要手动地为传入 <code>collate_fn</code> 的数据添加与 Memory Pinning 相关的代码，具体示例代码如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleCustomBatch</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data</span>):</span></span><br><span class="line">        transposed_data = <span class="built_in">list</span>(<span class="built_in">zip</span>(*data))</span><br><span class="line">        self.inp = torch.stack(transposed_data[<span class="number">0</span>], <span class="number">0</span>)</span><br><span class="line">        self.tgt = torch.stack(transposed_data[<span class="number">1</span>], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># custom memory pinning method on custom type</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pin_memory</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.inp = self.inp.pin_memory()</span><br><span class="line">        self.tgt = self.tgt.pin_memory()</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_wrapper</span>(<span class="params">batch</span>):</span></span><br><span class="line">    <span class="keyword">return</span> SimpleCustomBatch(batch)</span><br><span class="line"></span><br><span class="line">inps = torch.arange(<span class="number">10</span> * <span class="number">5</span>, dtype=torch.float32).view(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">tgts = torch.arange(<span class="number">10</span> * <span class="number">5</span>, dtype=torch.float32).view(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">dataset = TensorDataset(inps, tgts)</span><br><span class="line"></span><br><span class="line">loader = DataLoader(dataset, batch_size=<span class="number">2</span>, collate_fn=collate_wrapper,</span><br><span class="line">                    pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_ndx, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">    print(sample.inp.is_pinned())</span><br><span class="line">    print(sample.tgt.is_pinned())</span><br></pre></td></tr></table></figure>
</div>

<h2 class="title">DataLoader 源码解析</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于上一节对 PyTorch 提供的 <code>DataLoader</code> 有了功能性的认识后，本节我们将对其源码按顺序进行分析。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先在主进程中，我们会使用如上所示的代码对 <code>DataLoader</code> 进行遍历，此时会调用它的 <code>__iter__(self)</code> 魔法方法，该方法定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; &#x27;_BaseDataLoaderIter&#x27;:</span></span><br><span class="line">      <span class="keyword">if</span> self.persistent_workers <span class="keyword">and</span> self.num_workers &gt; <span class="number">0</span>:</span><br><span class="line">          <span class="comment"># 对于多进程数据加载的情况</span></span><br><span class="line">          <span class="keyword">if</span> self._iterator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">              <span class="comment"># 第一次发起遍历，则创建 iterator</span></span><br><span class="line">              self._iterator = self._get_iterator()</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">              <span class="comment"># 不是第一次发起遍历，则重置 iterator</span></span><br><span class="line">              self._iterator._reset(self)</span><br><span class="line">          <span class="keyword">return</span> self._iterator</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># 对于多进程数据加载的情况</span></span><br><span class="line">          <span class="keyword">return</span> self._get_iterator()</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在上面的代码中可以看见其调用了 <code>DataLoader</code> 类下的 <code>_get_iterator</code> 获取 Iterator，具体代码如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_iterator</span>(<span class="params">self</span>) -&gt; &#x27;_BaseDataLoaderIter&#x27;:</span></span><br><span class="line">      <span class="keyword">if</span> self.num_workers == <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">return</span> _SingleProcessDataLoaderIter(self)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          self.check_worker_number_rationality()</span><br><span class="line">          <span class="keyword">return</span> _MultiProcessingDataLoaderIter(self)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;可以看到其根据单/多进程数据读取的不同情况，返回了不同的迭代器，我们下面分情况进行讨论。

  <h3 class="title">迭代器父类</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先，不论是单进程数据加载所使用的迭代器 <code>_SingleProcessDataLoaderIter</code>，还是多进程数据加载所使用的迭代器 <code>_MultiProcessingDataLoaderIter</code>，他们都继承自 <code>_BaseDataLoaderIter</code> 基类，其源码如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_BaseDataLoaderIter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, loader: DataLoader</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">      self._dataset = loader.dataset</span><br><span class="line">      self._shared_seed = loader._get_shared_seed()</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._dataset, IterDataPipe):</span><br><span class="line">          shared_rng = torch.Generator()</span><br><span class="line">          shared_rng.manual_seed(self._shared_seed)</span><br><span class="line">          self._dataset = torch.utils.data.graph_settings.apply_shuffle_seed(self._dataset, shared_rng)</span><br><span class="line">      self._dataset_kind = loader._dataset_kind</span><br><span class="line">      self._IterableDataset_len_called = loader._IterableDataset_len_called</span><br><span class="line">      self._auto_collation = loader._auto_collation</span><br><span class="line">      self._drop_last = loader.drop_last</span><br><span class="line">      self._index_sampler = loader._index_sampler</span><br><span class="line">      self._num_workers = loader.num_workers</span><br><span class="line">      self._prefetch_factor = loader.prefetch_factor</span><br><span class="line">      <span class="comment"># for other backends, pin_memory_device need to set. if not set</span></span><br><span class="line">      <span class="comment"># default behaviour is CUDA device. if pin_memory_device is selected</span></span><br><span class="line">      <span class="comment"># and pin_memory is not set, the default behaviour false.</span></span><br><span class="line">      <span class="keyword">if</span> (<span class="built_in">len</span>(loader.pin_memory_device) == <span class="number">0</span>):</span><br><span class="line">          self._pin_memory = loader.pin_memory <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line">          self._pin_memory_device = <span class="literal">None</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">if</span> <span class="keyword">not</span> loader.pin_memory:</span><br><span class="line">              warn_msg = (<span class="string">&quot;pin memory device is set and pin_memory flag is not used then device pinned memory won&#x27;t be used&quot;</span></span><br><span class="line">                          <span class="string">&quot;please set pin_memory to true, if you need to use the device pin memory&quot;</span>)</span><br><span class="line">              warnings.warn(warn_msg)</span><br><span class="line"></span><br><span class="line">          self._pin_memory = loader.pin_memory</span><br><span class="line">          self._pin_memory_device = loader.pin_memory_device</span><br><span class="line">      self._timeout = loader.timeout</span><br><span class="line">      self._collate_fn = loader.collate_fn</span><br><span class="line">      self._sampler_iter = <span class="built_in">iter</span>(self._index_sampler)</span><br><span class="line">      self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()</span><br><span class="line">      self._persistent_workers = loader.persistent_workers</span><br><span class="line">      self._num_yielded = <span class="number">0</span></span><br><span class="line">      self._profile_name = <span class="string">&quot;enumerate(DataLoader)#&#123;&#125;.__next__&quot;</span>.<span class="built_in">format</span>(self.__class__.__name__)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>) -&gt; &#x27;_BaseDataLoaderIter&#x27;:</span></span><br><span class="line">      <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_reset</span>(<span class="params">self, loader, first_iter=<span class="literal">False</span></span>):</span></span><br><span class="line">      self._sampler_iter = <span class="built_in">iter</span>(self._index_sampler)</span><br><span class="line">      self._num_yielded = <span class="number">0</span></span><br><span class="line">      self._IterableDataset_len_called = loader._IterableDataset_len_called</span><br><span class="line">      self._shared_seed = loader._get_shared_seed()</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._dataset, IterDataPipe):</span><br><span class="line">          shared_rng = torch.Generator()</span><br><span class="line">          shared_rng.manual_seed(self._shared_seed)</span><br><span class="line">          self._dataset = torch.utils.data.graph_settings.apply_shuffle_seed(self._dataset, shared_rng)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_next_index</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">next</span>(self._sampler_iter)  <span class="comment"># may raise StopIteration</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_next_data</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>) -&gt; Any:</span></span><br><span class="line">      <span class="keyword">with</span> torch.autograd.profiler.record_function(self._profile_name):</span><br><span class="line">          <span class="keyword">if</span> self._sampler_iter <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">              <span class="comment"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span></span><br><span class="line">              self._reset()  <span class="comment"># type: ignore[call-arg]</span></span><br><span class="line">          data = self._next_data()</span><br><span class="line">          self._num_yielded += <span class="number">1</span></span><br><span class="line">          <span class="keyword">if</span> self._dataset_kind == _DatasetKind.Iterable <span class="keyword">and</span> \</span><br><span class="line">                  self._IterableDataset_len_called <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> \</span><br><span class="line">                  self._num_yielded &gt; self._IterableDataset_len_called:</span><br><span class="line">              warn_msg = (<span class="string">&quot;Length of IterableDataset &#123;&#125; was reported to be &#123;&#125; (when accessing len(dataloader)), but &#123;&#125; &quot;</span></span><br><span class="line">                          <span class="string">&quot;samples have been fetched. &quot;</span>).<span class="built_in">format</span>(self._dataset, self._IterableDataset_len_called,</span><br><span class="line">                                                                self._num_yielded)</span><br><span class="line">              <span class="keyword">if</span> self._num_workers &gt; <span class="number">0</span>:</span><br><span class="line">                  warn_msg += (<span class="string">&quot;For multiprocessing data-loading, this could be caused by not properly configuring the &quot;</span></span><br><span class="line">                               <span class="string">&quot;IterableDataset replica at each worker. Please see &quot;</span></span><br><span class="line">                               <span class="string">&quot;https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.&quot;</span>)</span><br><span class="line">              warnings.warn(warn_msg)</span><br><span class="line">          <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">  <span class="built_in">next</span> = __next__  <span class="comment"># Python 2 compatibility</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>) -&gt; int:</span></span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">len</span>(self._index_sampler)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getstate__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span> add limited pickling support for sharing an iterator</span></span><br><span class="line">      <span class="comment"># across multiple threads for HOGWILD.</span></span><br><span class="line">      <span class="comment"># Probably the best way to do this is by moving the sample pushing</span></span><br><span class="line">      <span class="comment"># to a separate thread and then just sharing the data queue</span></span><br><span class="line">      <span class="comment"># but signalling the end is tricky without a non-blocking API</span></span><br><span class="line">      <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;&#123;&#125; cannot be pickled&quot;</span>, self.__class__.__name__)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在 <code>_BaseDataLoaderIter</code> 中定义了 <code>__next__(self)</code> 函数，我们在主进程中使用 <code>for</code> 循环迭代 <code>DataLoader</code> 时，<code>for</code> 实际上就是通过不断调用该 <code>__next__(self)</code> 以获得下一批用于训练的 Batched Tensor，而 <code>__next__(self)</code> 则是调用 <code>_next_data()</code> 以获取相关数据，后者留给继承自 <code>_BaseDataLoaderIter</code> 的子类予以实现。

  <h3 class="title">单进程数据加载</h3>
  <div class="img" title="单进程数据加载调用链" label="img_single_process">
    <img src="./pic/single_process.png" width="90%" />
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在我们来看用于单进程数据加载的迭代器类，其基本调用关系如 <imgref>img_single_process</imgref> 所示。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;当我们在主程序的 <code>for</code> 循环中对 <code>DataLoader</code> 进行迭代时，首先其会调用 Iterable (i.e. <code>DataLoader</code>) 的 <code>__iter__(self)</code> 魔法方法以获得 Iterator，从上面展示过的程序中我们可以知道，在单进程数据加载的设定下，代表 Iterator 的类是  <code>_SingleProcessDataLoaderIter</code>，其定义如下所示:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_SingleProcessDataLoaderIter</span>(<span class="params">_BaseDataLoaderIter</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, loader</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(_SingleProcessDataLoaderIter, self).__init__(loader)</span><br><span class="line">        <span class="keyword">assert</span> self._timeout == <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> self._num_workers == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self._dataset_fetcher = _DatasetKind.create_fetcher(</span><br><span class="line">            self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_next_data</span>(<span class="params">self</span>):</span></span><br><span class="line">        index = self._next_index()  <span class="comment"># may raise StopIteration</span></span><br><span class="line">        data = self._dataset_fetcher.fetch(index)  <span class="comment"># may raise StopIteration</span></span><br><span class="line">        <span class="keyword">if</span> self._pin_memory:</span><br><span class="line">            data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;首先从 <code>_SingleProcessDataLoaderIter</code> 的初始化参数可以看到，其在父类 <code>_BaseDataLoaderIter</code> 的基础上定义了 <code>_dataset_fetcher</code>, 并传入 <code>_dataset</code>, <code>_auto_collation</code>, <code>_collate_fn</code> 等参数，该类用于根据指定的 Indices 来 Fetch 对应的 Samples，我们把他称为 $\text{Fetcher}$ 实体，在后面会对其源码进行分析。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;其次可以看见 <code>_SingleProcessDataLoaderIter</code> 实现了具体的 <code>_next_data()</code> 方法，其需要 <code>next_index()</code> 来获取要 Fetch 的 Samples 的 Indices，并将 Indices 传入 <code>_dataset_fetcher</code> 中以获取对应样本。下面我们分成「Indices 加载」和「Samples 加载」对源码进行分析。

  <h4 class="paragraph">Indices 加载</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上面看到的 <code>next_index()</code> 方法是在 <code>_SingleProcessDataLoaderIter</code> 的父类 <code>_BaseDataLoaderIter</code> 中定义的，我们把相关代码整理如下:

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_BaseDataLoaderIter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, loader: DataLoader</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        self._index_sampler = loader._index_sampler</span><br><span class="line">        self._sampler_iter = <span class="built_in">iter</span>(self._index_sampler)</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_next_index</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">next</span>(self._sampler_iter)  <span class="comment"># may raise StopIteration</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params">Generic[T_co]</span>):</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_auto_collation</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.batch_sampler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_index_sampler</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self._auto_collation:</span><br><span class="line">            <span class="keyword">return</span> self.batch_sampler</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.sampler</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;从上面的代码可以看出，根据 <code>DataLoader</code> 中是否启用了了 <code>batch_sampler</code>，<code>next_index()</code> 将对应地从 <code>DataLoader</code> 的 <code>batch_sampler</code> 或者 <code>sampler</code> 中迭代出 Indices。

  <h4 class="paragraph">Samples 加载</h3>

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DatasetKind</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  Map = <span class="number">0</span></span><br><span class="line">  Iterable = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">  @staticmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create_fetcher</span>(<span class="params">kind, dataset, auto_collation, collate_fn, drop_last</span>):</span></span><br><span class="line">      <span class="keyword">if</span> kind == _DatasetKind.Map:</span><br><span class="line">          <span class="keyword">return</span> _utils.fetch._MapDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">return</span> _utils.fetch._IterableDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在来看 Samples 加载的部分，也即 $\text{Fetcher}$ 实体。在 <code>_SingleProcessDataLoaderIter</code> 的构造函数中可以看到其调用了 <code>_DatasetKind.create_fetcher</code> 创建了 $\text{Fetcher}$ 实体，相关代码如上所示。根据数据集类型的不同，该函数会创建出 <code>_MapDatasetFetcher</code> 类型或者 <code>_IterableDatasetFetcher</code> 类型的 $\text{Fetcher}$ 实体，分别对应 Map-style 或者 Iterable-style 的数据集。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_MapDatasetFetcher</span>(<span class="params">_BaseDatasetFetcher</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, auto_collation, collate_fn, drop_last</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(_MapDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">self, possibly_batched_index</span>):</span></span><br><span class="line">      <span class="keyword">if</span> self.auto_collation:</span><br><span class="line">          data = [self.dataset[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> possibly_batched_index]</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          data = self.dataset[possibly_batched_index]</span><br><span class="line">      <span class="keyword">return</span> self.collate_fn(data)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 <code>_MapDatasetFetcher</code> 类型 $\text{Fetcher}$ 来说，如上所示，其定义的 <code>fetch()</code> 函数直接输入 Indices，作为 Map 的 Key，获得对应的样本，然后将获取的样本交给 <code>collate_fn</code> 进行输出前的整理。

  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_IterableDatasetFetcher</span>(<span class="params">_BaseDatasetFetcher</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dataset, auto_collation, collate_fn, drop_last</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(_IterableDatasetFetcher, self).__init__(dataset, auto_collation, collate_fn, drop_last)</span><br><span class="line">      self.dataset_iter = <span class="built_in">iter</span>(dataset)</span><br><span class="line">      self.ended = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">self, possibly_batched_index</span>):</span></span><br><span class="line">      <span class="keyword">if</span> self.ended:</span><br><span class="line">          <span class="keyword">raise</span> StopIteration</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self.auto_collation:</span><br><span class="line">          data = []</span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> possibly_batched_index:</span><br><span class="line">              <span class="keyword">try</span>:</span><br><span class="line">                  data.append(<span class="built_in">next</span>(self.dataset_iter))</span><br><span class="line">              <span class="keyword">except</span> StopIteration:</span><br><span class="line">                  self.ended = <span class="literal">True</span></span><br><span class="line">                  <span class="keyword">break</span></span><br><span class="line">          <span class="keyword">if</span> <span class="built_in">len</span>(data) == <span class="number">0</span> <span class="keyword">or</span> (self.drop_last <span class="keyword">and</span> <span class="built_in">len</span>(data) &lt; <span class="built_in">len</span>(possibly_batched_index)):</span><br><span class="line">              <span class="keyword">raise</span> StopIteration</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          data = <span class="built_in">next</span>(self.dataset_iter)</span><br><span class="line">      <span class="keyword">return</span> self.collate_fn(data)</span><br></pre></td></tr></table></figure>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 <code>_IterableDatasetFetcher</code> 类型 $\text{Fetcher}$ 来说，如上所示，其在构造函数内设置了对应 Dataset 初始的迭代器，在 <code>fetch()</code> 方法内利用该迭代器获取元素，输入 <code>fetch()</code> 的 Indices 其实已经没有多大作用了。

  <h3 class="title">多进程数据加载</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;
</div>


<div class="div_ref" id="ref_container"></div>

</body>

<!-- 圆圈数字 -->
<!--
⓪ ① ② ③ ④ ⑤ ⑥ ⑦ ⑧ ⑨ ⑩ ⑪ ⑫ ⑬ ⑭ ⑮ ⑯ ⑰ ⑱ ⑲ ⑳ ㉑ ㉒ ㉓ ㉔ ㉕ ㉖ ㉗ ㉘ ㉙ ㉚ ㉛ ㉜ ㉝ ㉞ ㉟ ㊱ ㊲ ㊳ ㊴ ㊵ ㊶ ㊷ ㊸ ㊹ ㊺ ㊻ ㊼ ㊽ ㊾ ㊿
-->

<!-- Flow Chart -->
<!--
Format see: https://mermaid-js.github.io/mermaid/#/flowchart
-->
<!-- <flowchart class="mermaid">
 Mermaid Flow Chart Code
</flowchart> -->

<!-- Sign Block -->
<!--
<noteblock>
A NOTE
</noteblock>

<queblock>
A QUESTION
</queblock>
-->

<!--图片、引用-->
<!-- 
<div class="img" title="img title" label="img_label" source="url">
  <img src="" height="" />
</div>

<imaging>img_label</imaging>
-->

<!--等式、引用-->
<!-- 
<div class="equation" label="equation_label">
</div>

<equation>equation_label</equation>
-->

<!--定理、引用、证明-->
<!-- 
<div class="theorm" label="theorm_label">
</div>

<theorm>theorm_label</theorm>

<div class="theorm_prove">
</div>
-->

<!--引用其它章节-->
<!-- 
<ref></ref> 
-->

<!--引用文献-->
<!-- 
<cite></cite> 
-->

<!--关键词-->
<!-- 
<def></def> 
-->

<!--醒目注意-->
<!-- 
<note></note> 
-->

<!--段落-->
<!--
<h3 class="paragraph">Paragraph Name</h3>
-->

<!--表格-->
<!--
<div class="table" title="Table Title" label="table_label">
  <table border="1" align="center" bgcolor="#FFFFFF">
    <tr>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
    <tr>
      <td>xxx</td>
      <td>xxx</td>
      <td>xxx</td>
    </tr>
  </table>
</div>
-->

<!--矩阵公式-->
<!--
<div class="cmath" align="center">
  `((1, 0),(1, 0))`
</div><br>
-->

<!--伪代码-->
<!--
<pre id="quicksort" style="display:hidden;">
  % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
  \begin{algorithm}
  \caption{Quicksort}
  \begin{algorithmic}
  \PROCEDURE{Quicksort}{$A, p, r$}
      % Add Here

      % 空行
      % \STATE \texttt{\\}
  \ENDPROCEDURE
  \end{algorithmic}
  \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("quicksort"));
</script>
-->
<!--
Latex 伪代码格式见: https://github.com/SaswatPadhi/pseudocode.js
-->

<!--图片-->
<!--
<div align="center">
  <img src="./pic/xxx.png" width=80%>
</div>
-->

<!--正文-->
<!--
<p>
&nbsp;&nbsp;&nbsp;&nbsp;公式：<span>`\overline{A}\overline{B}`</span>
</p>
-->
      </div>
      
      
      
    </div>
    
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Tech_System_And_Network/">TECH_SYSTEM_AND_NETWORK</a></li>
          <li>PYTORCH_DATALOADER</li>
        
  </ul>

    
    
    


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          المحتويات
        </li>
        <li class="sidebar-nav-overview">
          عام
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuobin Huang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhuobin Huang</p>
  <div class="site-description" itemprop="description">System Engineer</div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zobinHuang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zobinHuang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zobin1999@gmail.com" title="E-Mail → mailto:zobin1999@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.weibo.com/u/2861056530" title="Weibo → https:&#x2F;&#x2F;www.weibo.com&#x2F;u&#x2F;2861056530" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/HwangZobin" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;HwangZobin" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">粤ICP备2021044371号 </a>
  </div>

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-guitar"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuobin Huang</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'y8LMT8RtOsi4JsbYHtNm2J7U-gzGzoHsz',
      appKey     : 'Q0cSe4rR8Iwr0Gs60rwWBsYa',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
