<!DOCTYPE html>
<html lang="cn">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.zobinhuang.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":180,"display":"hide","padding":10,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#FF4136","save":"manual"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:type" content="website">
<meta property="og:title" content="线性回归和梯度下降">
<meta property="og:url" content="http://www.zobinhuang.com:10082/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/index.html">
<meta property="og:site_name" content="Zobin">
<meta property="og:description" content="MathJax &#x3D; {         tex: {             inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],             displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],             processEscapes: true,             process">
<meta property="og:locale">
<meta property="og:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/sl.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/gd.gif">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/gradient.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/gradient_2.png">
<meta property="og:image" content="http://www.zobinhuang.com:10082/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/pic/xxx.png">
<meta property="article:published_time" content="2022-04-09T16:45:38.976Z">
<meta property="article:modified_time" content="2022-04-09T16:45:38.976Z">
<meta property="article:author" content="Zhuobin Huang">
<meta property="article:tag" content="Zobin">
<meta property="article:tag" content="黄卓彬">
<meta property="article:tag" content="zobinHuang">
<meta property="article:tag" content="网络工程">
<meta property="article:tag" content="Networking Engineering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png">

<link rel="canonical" href="http://www.zobinhuang.com:10082/sec_learning/Algorithm/ML_2_Linear_Regression_And_Gradient_Descent/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'cn'
  };
</script>

  <title>线性回归和梯度下降 | Zobin
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Zobin" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zobin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Loves Tech & Tea</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-关于我">

    <a href="/sec_about/" rel="section"><i class="fa fa-address-card fa-fw"></i>关于我</a>

  </li>
        <li class="menu-item menu-item-知识库">

    <a href="/sec_learning/" rel="section"><i class="fa fa-book-open fa-fw"></i>知识库</a>

  </li>
        <li class="menu-item menu-item-进度">

    <a href="/sec_schedule/" rel="section"><i class="fa fa-calendar-alt fa-fw"></i>进度</a>

  </li>
        <li class="menu-item menu-item-独立音乐人">

    <a href="/sec_music/" rel="section"><i class="fa fa-music fa-fw"></i>独立音乐人</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="cn">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">线性回归和梯度下降
</h1>

<div class="post-meta">
  
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Algorithm/">ALGORITHM</a></li>
          <li>ML_2_LINEAR_REGRESSION_AND_GRADIENT_DESCENT</li>
        
  </ul>

</div>

</header>

      
      
      
      <div class="post-body">
          <head>
<!--导入样式表-->
<link rel="stylesheet" type="text/css" href="style/index.css">

<!--导入网页脚本-->
<script src="script/index.js"></script>

<!--支持伪代码显示-->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-chtml.js"
        integrity="sha256-3Fdoa5wQb+JYfEmTpQHx9sc/GuwpfC/0R9EpBki+mf8=" crossorigin>
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>

<!--支持网页公式显示-->    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<!--支持矩阵显示-->
<script type="text/javascript">
  run_maths = function() {
    if (document.querySelector('[class*="cmath"]') !== null) {
      if (typeof (mjax_path)=='undefined') { mjax_path='https://cdn.jsdelivr.net/npm/mathjax@2'; }
      if (typeof (mjax_config)=='undefined') { mjax_config='AM_CHTML'; }
      smjax = document.createElement ('script');
      smjax.setAttribute('src',`${mjax_path}/MathJax.js?config=${mjax_config}`);
      smjax.setAttribute('async',true);
      document.getElementsByTagName('head')[0].appendChild(smjax);
    }
  };
  if (document.readyState === 'loading') {  
    window.addEventListener('DOMContentLoaded', run_maths); 
  } else { 
    run_maths(); 
  }
</script>
</head>

<body onload="load_page()">

<div align="center" class="div_indicate_source">
  <h4>⚠ 转载请注明出处：<font color="red"><i>作者：ZobinHuang，更新日期：Apr.4 2022</i></font></h4>
</div>
<div class="div_licence">
  <br>
  <div align="center">
      <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="知识共享许可协议" style="border-width:0; margin-left: 20px; margin-right: 20px;" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a>
  </div>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;本<span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" rel="dct:type">作品</span>由 <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"><b>ZobinHuang</b></span> 采用 <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><font color="red">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</font></a> 进行许可，在进行使用或分享前请查看权限要求。若发现侵权行为，会采取法律手段维护作者正当合法权益，谢谢配合。
  </p>
</div>
<br>
<div class="div_catalogue">
  <div align="center">
    <h1> 目录 </h1>
    <p>
    <font size="3px">有特定需要的内容直接跳转到相关章节查看即可。</font>
  </div>
  <div class="div_load_catalogue_alert" id="load_catalogue_alert">正在加载目录...</div>
  <div class="div_catalogue_container" id="catalogue_container">
  </div>
</div><br>

<!-- Start your post here -->
<h2 class="title">Linear Regression</h2>
<div class="div_learning_post">
  <h3 class="title">Supervised Learning</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Linear Regression (LR) 是 Supervised Learning 的一种简单情况。首先我们回顾一下 Supervised Learning 的基本流程:

  <div align="center">
    <img src="./pic/sl.png" width=500px>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们给出一个 <def>Training Set</def>，里面包含了 `m` 条 <def>Samples</def> `(X,Y)=[(X^{(0)},Y^{(0)}),...,(X^{(m-1)},Y^{(m-1)})]`，每一条 Sample `(X^{(i)},Y^{(i)})`  中都有对应的 <def>Features</def> `X^{(i)}` 以及其对应的 <def>Label</def> `Y^{(i)`。注意到 `X^{(i)}` 是 `n` 维的，即 

  <div class="equation">
  `X^{(i)}=[x_{0}^{(i)},x_{1}^{(i)},...,x_{n-1}^{(i)}]`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;`y^{(i)}` 是 `l` 维的，即

  <div class="equation">
  `Y^{(i)}=[y_{0}^{(i)},y_{1}^{(i)},...,y_{l-1}^{(i)}]`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;值得注意的是，对于 LR 问题来说，`l=1`，也即每一个 Sample 的 Label 都是一个连续的数值。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;这些 Samples 将作为 <def>Learning Algorithm</def> 的输入，后者需要输出一个 <def>Hypothesis</def> `h(X;\Theta)`，注意到 `h` 带有 <def>Parameters</def> `\Theta`，其任务是对新的输入向量 `X^{(n ew)}` 输出一个预测值 `Y^{(n ew)}`。

  <h3 class="title">Linear Regression</h3>
  <h4 class="title">`h` 的形式</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于上述流程，我们首先关心的问题是 `h` 的形式。在 LR 问题下，我们可以把 `h` 表示如下:

  <div class="equation">
    `\hat{Y}^{(i)} = h(X^{(i)};\Theta) = \theta_{b} + \sum_{j=0}^{n-1}\theta_jx_j^{(i)}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;其中 `n` 表示了输入向量 `X` 的维度，对应的也是参数 `\Theta` 的维度; `\theta_{b}` 是一个偏置值，我们可以给 `\Theta` 和 `X` 多加一个维度来合并这个偏置值，这样一来 `x_n` 就恒等为 `1`。化简如下：

  <div class="equation">
    `\hat{Y}^{(i)} = h(X^{(i)};\Theta) = \sum_{j=0}^{n}\theta_jx_j^{(i)}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;再次强调，`X`, `Y` 和 `\Theta` 都是向量，可以表示如下:

  <div class="equation">
  <div class="cmath" align="center">
    <p>`\Theta = ((\theta_0),(\theta_1),(...),(\theta_{n}))`，`X = ((x_0),(x_1),(...),(x_{n}))`，`Y = ((y_0),(y_1),(...),(y_{l-1}))`  
  </div>
  </div>

  <h4 class="title">如何选取参数 `\theta`</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在有了 `h(X;\Theta)` 的形式之后，我们接下来关心的问题就是如何选取 `\Theta`，这也是 Leanring Algorithm 本质上在干的事情。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;直觉告诉我们，`h(X;\Theta)` 所应用的参数 `\Theta`，应该使得 `\forall X^{(i)}\in X`，`h(X^{(i)};\Theta) \approx Y^{(i)}`，也即 `h(X;\Theta)` 输出的结果应该尽可能得和 Training Set 中各个 Sample 的 Label 接近。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于这个直觉，我们定义一个简单的 <def>Lost Function</def>:

  <div class="equation">
  `\mathcal{J}(\Theta) = \sum_{i=0}^{m-1}(h(X^{(i)};\Theta)-Y^{(i)})^2`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;值得注意的是，在上面这个式子中，`\Theta` 是自变量，而 Training Set 中的各个 Sample `(X^{(i)},Y^{(i)})` 则是参数，`\mathcal{J}(\Theta)` 是因变量。这与我们上面讨论的 <equation>4</equation> 中 `X`, `Y` 和 `\Theta` 的关系有所不同。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;为了后面计算方便，我们在前面加上系数:

  <div class="equation">
  `\mathcal{J}(\Theta) = \frac{1}{2}\sum_{i=0}^{m-1}(h(X^{(i)};\Theta)-Y^{(i)})^2`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;因此，Learning Algorithm 的工作是：找到使得 `J(\Theta)` 最小的 `\Theta` 值，也即:

  <div class="equation">
  `\Theta=` argmin`[J(\Theta)]`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;在明确 Learning Algorithm 的任务后，我们在 <ref>gd</ref> 一节中将看到我们是如何实现上面这个目标的。
</div>

<h2 class="title">Batch/Stochastic Gradient Descent</h2>
<div class="div_learning_post">
  <label class="title">gd</label>
  <h3 class="title">Batch Gradient Descent</h3>

  <h4 class="title">初步思路</h4>
  <div align="center">
    <img src="./pic/gd.gif" width=500px>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;一个初步的思路是，我们首先随机选定一组参数 `\Theta`，比如 `\Theta={\theta_0=0,\theta_1=0,...,\theta_{n-1}=0}`，然后我们不断改变 `\Theta` 来降低 `\mathcal{J}(\theta)` 的值。这个思路在输入向量是二维的时候，可以采用如上所示的图来表示。

  <div class="theorm_prove">
  <div align="center"><h4>为什么不联立方程求出 `J(\theta)` 的最小值点?</h4></div>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;TODO
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;既然我们采用了逐步下降的办法，我们就需要使用到数学工具 —— 梯度。下面我们对相关数学知识进行回顾。

  <h4 class="title">求导的本质</h4>
  <div align="center">
    <img src="./pic/gradient.png" width=300px>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于单变量连续函数 `f(x)` 来说，如果我们对其自变量 `x` 进行求导:

  <div class="equation">
  `f'(x) = \frac{df(x)}{dx}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们最终实际上会得到一个向量 `(1,f'(x))`，这个向量指定了当自变量取值为 `x^{(i)}` 时，函数的增长方向向量 `(1,f'(x^{(i)}))`，如上图所示。这个增长方向向量也可以使用我们平时常说的 "斜率" 来表示，也即 `\tan\beta=\frac{f'(x^{(i)})}{1}=f'(x^{(i)})`，我们称这个 "斜率" 为 `f(x)` 的 <def>梯度 (Gradient)</def>，记为 `\nablaf(x)=f'(x)`。梯度 `\nablaf(x)` 反映了自变量 `x` 发生的微小变化给 `f(x)` 带来的影响。

  <div align="center">
    <img src="./pic/gradient_2.png" height=400px>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;同样地，对于多维连续函数 `f(X)` 来说，如果我们对其自变量 `X=[x_1,x_2,...,x_n]` 求偏导数，我们会得到在各个方向上的 `f(X)` 的增长方向向量 `(1,f'(x_1))`, `(1,f'(x_2))`, ..., `(1,f'(x_n))`。如上图所示，以二维的情况为例，若自变量在各个增长方向上的速率相同，则我们可以对各个增长方向向量进行合成，并求出总的梯度为

  <div class="equation">
  `\frac{\nablaf(x_1)+\nablaf(x_2)}{\sqrt{2}}=\frac{\frac{\partial f(x)}{\partial x_0}+\frac{\partial f(x)}{\partial x_1}}{\sqrt{2}}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;如果自变量在各个增长方向上的速率不同，则就没有以上形式的结论。我们可以把总的梯度表示为由各个方向上的梯度组成的向量，也即:

  <div class="equation">
  `f'(X)` `=` `\nablaf(X)` `=` `[\nablaf(x_1),\nablaf(x_2)]^{T}` `=` `[\frac{\partialf(x)}{\partial x_1},\frac{\partialf(x)}{\partial x_2}]^{T}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;这样一来，我们如果知道了 `X` 在各个分量 `x_p` 上的变化率 `\alpha_p`，我们就能求出来 `f'(X)` 的真实梯度:

  <div class="equation">
  `[\alpha_1, \alpha_2] \cdot \nablaf(X)` `=` `[\alpha_1, \alpha_2] \cdot [\nablaf(x_1),\nablaf(x_2)]^{T}` `=` `\alpha_1\nablaf(x_1) + \alpha_2\nablaf(x_2)`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;可见，单变量的连续函数的梯度向量实际上是一种特殊情况，其向量只有一个分量，因此是一个标量。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;拓展到更多维的情况下，我们得到:

  <div class="equation">
  `\nablaf(X)``=``[\nablaf(x_1),\nablaf(x_2),...,\nablaf(x_n)]^{T}``=``[\frac{\partialf(x)}{\partialx_1},\frac{\partialf(x)}{\partialx_2},...,\frac{\partialf(x)}{\partialx_n}]^{T}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;梯度中的每一个分量 `\nablaf(x_i)` 代表着: 自变量 `X` 的分量 `x_i` 发生的微小变化给 `f(x)` 带来的影响。

  <h4 class="title">Batch Gradient Descent</h4>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;回到我们的问题: 采用逐渐下降的办法来逼近 `\mathcal{J}(\Theta)` 的最小值。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上面我们看到了，求解出 `f'(X^{(i)})`，我们就可以得到在 `X^{(i)}=(x_0^{(i)}, x_1^{(i)}, ..., x_{n-1}^{(i)})` 这个点的增长方向向量，体现在各个分量 `x_p` 的梯度值 `\nablaf(x_p^{(i)}) = \frac{\partialf(X)}{\partialx_p}|_{x_p=x_p^{(i)}}`。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;同样的道理，求解出 `\mathcal{J}'(\Theta^{(i)})`，我们就可以得到在 `\Theta^{(i)}=(\theta_0^{(i)}, \theta_1^{(i)}, ..., \theta_{n-1}^{(i)})` 这个点的增长方向向量，体现在各个分量 `\theta_j` 的梯度值 `\nabla\mathcal{J}(\theta_j^{(i)}) = \frac{\partial\mathcal{J}(\Theta)}{\partial\theta_j}|_{\theta_j=\theta_j^{(i)}}`。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;但是由于我们想要的是让 `\mathcal{J}(\Theta)` 减小的效果，所以真正有用的是与「增长方向向量」方向相反的向量，因此我们实际上要的是:

  <div class="equation">
  `-\nabla\mathcal{J}(\theta_j) = -\frac{\partial\mathcal{J}(\Theta)}{\partial\theta_j}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;因此，对于每一个参数，我们应用:

  <div class="equation">
  `\theta_j := \theta_j - \alpha \frac{\partial \mathcal{J}(\Theta)}{\partial \theta_j}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;其中 `\alpha` 称为 <def>Learning Rate</def>，可以理解为每次向着梯度的反方向步进的长度。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于梯度项 `\frac{\partial \mathcal{J}(\Theta)}{\partial \theta_j}`，让我们结合 <equation>4</equation> 和 <equation>7</equation>，代入上述等式求解。

  <div class="equation">
  `\frac{\partial \mathcal{J}(\Theta)}{\partial \theta_j}`<br>
  `= \frac{\partial}{\partial \theta_j}{\frac{1}{2}\sum_{i=0}^{m-1}(h(X^{(i)};\Theta)-Y^{(i)})^2}`<br>
  `=[h(X^{(0)};\Theta)-Y^{(0)}] \cdot \frac{\partial}{\partial \theta_j}[h(X^{(0)};\Theta)-Y^{(0)}] + ...`<br>
  <note>(注意到 LR 问题中，`Y^{(i)}` 实际上是一个标量，即 `Y^{(i)}=y^{(i)}`，因此)</note><br>
  `=[\theta_0x_0^{(0)}+...\theta_nx_n^{(0)}-y^{(0)}] \cdot \frac{\partial}{\partial \theta_j}[\theta_0x_0^{(0)}+...\theta_nx_n^{(0)}-y^{(0)}]+...`<br>
  `=[\theta_0x_0^{(0)}+...\theta_nx_n^{(0)}-y^{(0)}] \cdot x_j^{(0)} + ...`<br>
  `=\sum_{i=0}^{m-1}[\theta_0x_0^{(i)}+...\theta_nx_n^{(i)}-y^{(i)}] \cdot x_j^{(i)}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 <equation>15</equation> 和 <equation>16</equation>，我们可以得到更新每一个 `\theta_j` 的公式:

  <div class="equation">
  `\theta_j := \theta_j - \alpha \cdot {\sum_{i=0}^{m-1}[\theta_0x_0^{(i)}+...\theta_nx_n^{(i)}-y^{(i)}] \cdot x_j^{(i)}}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于 <equation>17</equation>，我们可以得到使用梯度下降迭代得到最终 `\Theta` 的算法:

  <pre id="gd" style="display:hidden;">
    \begin{algorithm}
    \caption{Batch Gradient Descent}
    \begin{algorithmic}
    \INPUT Training Set $(X,Y)$
    \STATE $\Theta = \overrightarrow{0}$
    \WHILE{ Not Coverage }
      \FOR{$\theta_j$ in $\Theta$}
      \STATE EQUATION\_17($\theta_j$)
      \ENDFOR
    \ENDWHILE
    \end{algorithmic}
    \end{algorithm}
  </pre>
  <script>
      pseudocode.renderElement(document.getElementById("gd"));
  </script>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在我们已经完成了第一个学习算法，观察 <equation>17</equation>，可以发现我们在每个参数 `\theta_j` 的每步更新时，我们需要结合所有 `m` 条 Samples 来帮助我们计算梯度，因此我们把上述算法称为 <def>Batch Gradient Descent</def> (BGD, 批梯度下降)。

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;思考一下，假设现在一共有一亿条 Samples，`\Theta` 的维度是五万个参数，可以想像，使用 BGD 迭代来更新参数的方法将会十分缓慢。因此，我们下面介绍一种改进的方法。

  <h3 class="title">Stochastic Gradient Descent</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;上面我们发现导致 BGD 缓慢的原因是因为，每轮迭代中算每个参数 `\theta_j` 的梯度需要使用到整个数据集，因此我们对 BGD 进行修改: 每轮迭代中算每个参数 `\theta_j` 的梯度时，只随机选取一条 Sample 来进行计算 ，也即:

  <div class="equation">
  `\theta_j := \theta_j - \alpha \cdot [\theta_0x_0^{(i)}+...\theta_nx_n^{(i)}-y^{(i)}] \cdot x_j^{(i)}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;这样一来，虽然我们不能保证每轮迭代中，对 `\theta_j` 的更新都是朝着最优的方向前进，但是从平均的角度来看 (i.e. 我们最终还是把整个数据集喂进了学习算法)，我们最终会朝着最优的方向前进。修改过的算法如下:

  <pre id="gd" style="display:hidden;">
    \begin{algorithm}
    \caption{Stochastic Gradient Descent}
    \begin{algorithmic}
    \INPUT Training Set $(X,Y)$
    \STATE $\Theta = \overrightarrow{0}$
    \WHILE{ Not Coverage }
      \FOR{$\theta_j$ in $\Theta$}
      \STATE EQUATION\_18($\theta_j$)
      \ENDFOR
    \ENDWHILE
    \end{algorithmic}
    \end{algorithm}
  </pre>
  <script>
      pseudocode.renderElement(document.getElementById("gd"));
  </script>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们把修改过的算法称为 <def>Stochastic Gradient Descent</def> (SGD, 随机梯度下降)。
</div>

<h2 class="title">Normal Equation</h2>
<div class="div_learning_post">
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对于 LR 问题，除了采用其它 Learning Algorithm 都会使用的梯度下降算法以外，我们还可以使用矩阵方法来实现一步完成对 <equation>8</equation> 的求解。

  <h3 class="title">Notation</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;我们说一个 <def>矩阵函数</def> 把一个矩阵 `A` 映射称为一个实数。比如如下函数：

  <div class="equation">
  <div class="cmath" align="center">
    `f{((A_{11},A_{12}),(A_{21},A_{22}))} = A_{11} + A_{12}^2`
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;对一个上述矩阵函数求导，我们规定形式为:

  <div class="equation">
  <div class="cmath" align="center">
    `\nabla_{A}f(A)=((\frac{\partial f}{\partial A_{11}},\frac{\partial f}{\partial A_{12}}),(\frac{\partial f}{\partial A_{21}},\frac{\partial f}{\partial A_{22}}))`
    `=((1,2A_{12}),(0,0))`
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;另外，对于 `n \times n` 的 Square Matrix (方块矩阵) `A`，<def>Trace</def> (迹) 的定义如下:

  <div class="equation">
  `trA=\sum_{i=1}^{n}A_{ii}`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;Trace 具有如下的属性:

  <div class="equation">
  对于 `n \times n` 的矩阵 `A`, `B`, `C`，我们有:
  <ol>
    <li>`trA = trA^T`</li>
    <li>`tr(A+B) = trA + trB`</li>
    <li>`tr(a \cdot A) = a \cdot trA`</li>
    <li>`trAB = trBA`</li>
    <li>`trABC = trCAB = trBCA`</li>
    <li>`......`</li>
  </ol>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;和梯度结合起来，不加证明地，我们给出 Trace 如下的属性:

  <div class="equation">
  对于 `n \times n` 的矩阵 `A`, `B` 我们有:
  <ol>
    <li>`\nabla_AtrAB = B^T`</li>
    <li>`\nabla_{A^T}f(A) = (\nabla_{A}f(A))^T`</li>
    <li>`\nabla_AtrABA^TC = CAB + C^TAB^T`</li>
    <li>`\nabla_A|A| = |A|(A^{-1})^T` [p.s. `|A|` 指的是 `A` 的 Determinant (行列式) ]</li>
  </ol>
  </div>

  <h3 class="title">Normal Equation</h3>
  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在让我们来关心我们如何基于对矩阵求解梯度的方式来一步解得最优参数。回顾 <equation>5</equation>，我们可以把各个式子表示为：

  <div class="equation">
  <div class="cmath" align="center">
  `X=((—(X^{(0)})^T—),(—(X^{(1)})^T—),(...),(—(X^{(m)})^T—))`，`\Theta=((\theta_0),(\theta_1),(...),(\theta_m))`，`Y=((Y^{(0)}),(Y^{(1)}),(...),(Y^{(m)}))`
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;因此有:

  <div class="equation">
  <div class="cmath" align="center">
  `X\Theta - Y``=``(((X^{(0)})^T\Theta),((X^{(1)})^T\Theta),(...),((X^{(m)})^T\Theta))-((Y^{(0)}),(Y^{(1)}),(...),(Y^{(m)}))``=``((h(X^{(0)};\Theta)-Y^{(0)}),(h(X^{(1)};\Theta)-Y^{(1)}),(...),(h(X^{(m)};\Theta)-Y^{(m)}))`
  </div>
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;回顾 <equation>7</equation>，我们可以把式子表示为：

  <div class="equation">
  `\mathcal{J}(\Theta)=\frac{1}{2}(X\Theta-Y)^T(X\Theta-Y)`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;现在我们把 Lost Function `\mathcal{J}(\Theta)` 表达为了矩阵的形式，结合上面我们讨论过的矩阵的一些性质，我们可以得到:

  <div class="equation">
  `\nabla_\Theta\mathcal{J}(\Theta)`<br>
  `= \nabla_\Theta{\frac{1}{2}(X\Theta-Y)^T(X\Theta-Y)}`<br>
  `= \frac{1}{2}\nabla_\Theta{(\Theta^TX^T-Y^T)(X\Theta-Y)}`<br>
  `= \frac{1}{2} \nabla_\Theta (\Theta^TX^TX\Theta - \Theta^TX^TY - Y^TX\Theta+Y^TY)`<br>
  `= \frac{1}{2} [X^TX\Theta+X^TX\Theta-X^TY-X^TY]`<br>
  `= X^TX\Theta-X^TY`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;为了得到全局最优解，我们让上面得到的最终结果等于零向量，即:

  <div class="equation">
  `X^TX\Theta = X^TY`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;<equation>28</equation> 即 <def>Normal Equation</def>，进一步我们得到:

  <div class="equation">
  `\Theta=(X^TX)^{-1}X^TY`
  </div>

  <p>
  &nbsp;&nbsp;&nbsp;&nbsp;基于这个式子，我们就可以直接得出最优的参数 `\Theta`，而不用使用迭代的方法来逐渐逼近最优解。
</div>

<div class="div_ref" id="ref_container"></div>

</body>


<!--定理、引用、证明-->
<!-- 
<div class="theorm" id="theorm_id">
</div>

<theorm>theorm_id</theorm>

<div class="theorm_prove">
</div>
-->

<!--引用其它章节-->
<!-- 
<ref></ref> 
-->

<!--引用文献-->
<!-- 
<cite></cite> 
-->

<!--关键词-->
<!-- 
<def></def> 
-->

<!--醒目注意-->
<!-- 
<note></note> 
-->

<!--表格-->
<!--
<table border="1" align="center" bgcolor="#FFFFFF">
  <caption>表格</caption>
  <tr>
    <th>A</th>
    <th>B</th>
    <th>C</th>
  </tr>
  <tr>
    <td>xxx</td>
    <td>xxx</td>
    <td>xxx</td>
  </tr>
</table>
-->

<!--矩阵公式-->
<!--
<div class="cmath" align="center">
  `((1, 0),(1, 0))`
</div><br>
-->

<!--伪代码-->
<!--
<pre id="quicksort" style="display:hidden;">
  % This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
  \begin{algorithm}
  \caption{Quicksort}
  \begin{algorithmic}
  \PROCEDURE{Quicksort}{$A, p, r$}
      % Add Here

      % 空行
      % \STATE \texttt{\\}
  \ENDPROCEDURE
  \end{algorithmic}
  \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("quicksort"));
</script>
-->
<!--
Latex 伪代码格式见: https://github.com/SaswatPadhi/pseudocode.js
-->

<!--图片-->
<!--
<div align="center">
  <img src="./pic/xxx.png" width=80%>
</div>
-->

<!--正文-->
<!--
<p>
&nbsp;&nbsp;&nbsp;&nbsp;公式：<span>`\overline{A}\overline{B}`</span>
</p>
-->
      </div>
      
      
      
    </div>
    
  <ul class="breadcrumb">
          
            <li><a href="/sec_learning/">SEC_LEARNING</a></li>
            <li><a href="/sec_learning/Algorithm/">ALGORITHM</a></li>
          <li>ML_2_LINEAR_REGRESSION_AND_GRADIENT_DESCENT</li>
        
  </ul>

    
    
    


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuobin Huang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhuobin Huang</p>
  <div class="site-description" itemprop="description">System Engineer</div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zobinHuang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zobinHuang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zobin1999@gmail.com" title="E-Mail → mailto:zobin1999@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.weibo.com/u/2861056530" title="Weibo → https:&#x2F;&#x2F;www.weibo.com&#x2F;u&#x2F;2861056530" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/HwangZobin" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;HwangZobin" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">粤ICP备2021044371号 </a>
  </div>

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-guitar"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuobin Huang</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'y8LMT8RtOsi4JsbYHtNm2J7U-gzGzoHsz',
      appKey     : 'Q0cSe4rR8Iwr0Gs60rwWBsYa',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
